"""LASTTTTTTTTTDM.ipynb

Automatically generated by Colab.
"""

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder

import seaborn as sns
import matplotlib.pyplot as plt

# ==========================
# LOAD DATA
# ==========================

Data_set = pd.read_csv("Project/Train data.csv")

# Remove Loan_ID because it's not a predictive feature
if 'Loan_ID' in Data_set.columns:
    Data_set.drop(columns=['Loan_ID'], inplace=True)

print(Data_set)

# Ensure Credit_History is treated as categorical early
if 'Credit_History' in Data_set.columns:
    Data_set['Credit_History'] = Data_set['Credit_History'].astype('object')

# ==========================
# CHECK SKEWNESS OF NUMERIC FEATURES
# ==========================

numeric_cols_for_plot = Data_set.select_dtypes(include=np.number).columns.tolist()

if 'Loan_Status' in numeric_cols_for_plot:
    numeric_cols_for_plot.remove('Loan_Status')

print("Plotting distributions for numeric columns to check for skewness:")

for col in numeric_cols_for_plot:
    plt.figure(figsize=(8, 4))
    sns.histplot(Data_set[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# ==========================
# HANDLE MISSING VALUES
# ==========================

number_cols = Data_set.select_dtypes(exclude=['object']).columns.tolist()
df_num = Data_set[number_cols]

imp_mean = SimpleImputer(strategy='mean')
df_num = pd.DataFrame(imp_mean.fit_transform(df_num), columns=number_cols)

print("\nNumeric columns after imputation:")
print(df_num.head())

category_cols = Data_set.select_dtypes(include=['object']).columns.tolist()
df_cat = Data_set[category_cols]

imp_mode = SimpleImputer(strategy='most_frequent')
df_cat = pd.DataFrame(imp_mode.fit_transform(df_cat), columns=category_cols)

print("\nCategorical columns after imputation:")
print(df_cat.head())

# Put data back together
Data_set[number_cols] = df_num
Data_set[category_cols] = df_cat

print("\nCombined dataset:")
print(Data_set.head())

# ==========================
# REMOVE OUTLIERS (NO SCALING USED)
# ==========================

def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower) | (df[column] > upper)]
    return outliers, lower, upper

print("\n=== OUTLIER DETECTION RESULTS ===")

skip_cols = ['Loan_Amount_Term']  # Avoid removing too many rows

initial_shape = Data_set.shape
print("Shape before outlier removal:", initial_shape)

for col in number_cols:
    if col in skip_cols:
        continue
    if Data_set[col].nunique() > 5:
        outliers, lower, upper = detect_outliers_iqr(Data_set, col)
        print(f"\nColumn: {col}")
        print(f"Lower bound: {lower:.2f} | Upper bound: {upper:.2f}")
        print(f"Outliers detected: {len(outliers)}")

        plt.figure(figsize=(6, 2))
        sns.boxplot(x=Data_set[col], color='red')
        plt.title(f"Boxplot for {col}")
        plt.show()

        Data_set = Data_set.drop(outliers.index).copy()
        print(f"Removed {len(outliers)} outliers from column: {col}")

print("\nShape after outlier removal:", Data_set.shape)

# ==========================
# ENCODING
# ==========================

cat_cols = Data_set.select_dtypes(include=['object']).columns.tolist()
print("\nCategorical columns before encoding:", cat_cols)

# Convert "3+" to integer for Dependents
if 'Dependents' in Data_set.columns:
    Data_set['Dependents'] = Data_set['Dependents'].replace('3+', 3)
    Data_set['Dependents'] = pd.to_numeric(Data_set['Dependents'], errors='coerce')

binary_cols = ['Gender', 'Married', 'Education', 'Self_Employed', 'Loan_Status', 'Credit_History']

le = LabelEncoder()
for col in binary_cols:
    if col in Data_set.columns:
        Data_set[col] = le.fit_transform(Data_set[col])

# One-Hot encoding for multi-category
Data_set = pd.get_dummies(Data_set, columns=['Property_Area'], drop_first=True)

print("\nEncoding complete!")
print(Data_set.head())

for col in ['Property_Area_Semiurban', 'Property_Area_Urban']:
    if col in Data_set.columns:
        Data_set[col] = Data_set[col].astype(int)

# ==========================
# CORRELATION HEATMAP
# ==========================

corr_matrix = Data_set.select_dtypes(include=['number']).corr()

plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Heatmap of All Encoded Features")
plt.show()

target_corr = corr_matrix['Loan_Status'].sort_values(ascending=False)
print("Correlation with Loan_Status:")
print(target_corr)

# ==========================
# SAVE FINAL CLEANED DATASET
# ==========================

Data_set['Loan_Status'] = Data_set['Loan_Status'].astype(int)
Data_set = Data_set.reset_index(drop=True)

Data_set.to_csv("Cleaned_Loan_Train_Data_NoScaling.csv", index=False)

print("\n===============================")
print("Final cleaned dataset created WITHOUT SCALING:")
print("Saved as: Cleaned_Loan_Train_Data_NoScaling.csv")
print("Shape:", Data_set.shape)
print("===============================")
